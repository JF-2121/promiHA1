{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JF-2121/promiHA1/blob/master/exercise3_student.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9699f2ad",
      "metadata": {
        "id": "9699f2ad"
      },
      "source": [
        "# Promi Exercise Sheet 3: Estimation and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e671ef5",
      "metadata": {
        "id": "6e671ef5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib\n",
        "from matplotlib import pyplot as plt\n",
        "from matplotlib.patches import Ellipse"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37142144",
      "metadata": {
        "id": "37142144"
      },
      "source": [
        "# Part 1: Regression\n",
        "In this exercise, you will work on linear regression with polynomial features where we model the function $f(\\bm{x})$ as\n",
        "\n",
        "$$f(x) = \\bm{w}^\\intercal \\phi(x).$$\n",
        "\n",
        "The true model is a polynomial of degree 3\n",
        "\n",
        "$$f(x) = 0.5 + (2x - 0.5)^2 + x^3$$\n",
        "\n",
        "We further introduce noise into the system by adding a noise term $\\varepsilon_i$ which is sampled from a Gaussian distribution\n",
        "\n",
        "$$y = f(x) + \\varepsilon_i, \\quad\\varepsilon_i\\sim \\mathcal{N}(0, \\sigma^2).$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddd1e227",
      "metadata": {
        "id": "ddd1e227"
      },
      "outputs": [],
      "source": [
        "def f(x):\n",
        "  \"\"\"The true polynomial that generated the data D\n",
        "  Args:\n",
        "    x: Input data\n",
        "  Returns:\n",
        "    Polynomial evaluated at x\n",
        "  \"\"\"\n",
        "  return x ** 3 + (2 * x - .5) ** 2 + .5\n",
        "\n",
        "def generate_data(n, minval, maxval, variance=1., train=False, seed=0):\n",
        "  \"\"\"Generate the datasets. Note that we don't want to extrapolate,\n",
        "  and such, the eval data should always lie inside of the train data.\n",
        "  Args:\n",
        "    n: Number of datapoints to sample. n has to be atleast 2.\n",
        "    minval: Lower boundary for samples x\n",
        "    maxval: Upper boundary for samples x\n",
        "    variance: Variance or squared standard deviation of the model\n",
        "    train: Flag deciding whether we sample training or evaluation data\n",
        "    seed: Random seed\n",
        "  Returns:\n",
        "    Tuple of randomly generated data x and the according y\n",
        "  \"\"\"\n",
        "  # Set numpy random number generator\n",
        "  rng = np.random.default_rng(seed)\n",
        "\n",
        "  # Sample data along the x-axis\n",
        "  if train:\n",
        "    # We first sample uniformly on the x-Axis\n",
        "    x = rng.uniform(minval, maxval, size=(n - 2,))\n",
        "    # We will sample on datapoint beyond the min and max values to\n",
        "    # guarantee that we do not extrapolate during the evaluation\n",
        "    margin = (maxval - minval) / 100\n",
        "    min_x = rng.uniform(minval - margin, minval, (1,))\n",
        "    max_x = rng.uniform(maxval, maxval + margin, (1,))\n",
        "    x = np.concatenate((x, min_x, max_x))\n",
        "  else:\n",
        "    x = rng.uniform(minval, maxval, size=(n,))\n",
        "  eps = rng.standard_normal(n)\n",
        "\n",
        "  #\n",
        "  return x, f(x) + variance * eps"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "81fc1e17",
      "metadata": {
        "id": "81fc1e17"
      },
      "source": [
        "## Question 1.1: Least Squares Linear Regression\n",
        "To carry out regression, we first need to define the basis functions $\\phi(\\mathbf{x})$. In this task we would like to use polynomial features of degree $k$. Furthermore, we implement ridge regression as we want to counteract overfitting.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2b1bd04",
      "metadata": {
        "id": "a2b1bd04"
      },
      "outputs": [],
      "source": [
        "def polynomial_features(x, degree):\n",
        "  \"\"\"\n",
        "  A polynomial feature function of degree n.\n",
        "  :param x: input of size (N, D)\n",
        "  :param degree: polynomial degree\n",
        "  :return: Polynomial features evaluated at x of dim (degree, N)\n",
        "  \"\"\"\n",
        "  #### YOUR SOLUTION HERE\n",
        "\n",
        "def fit_w(x, y, lam, degree):\n",
        "  \"\"\"\n",
        "  Fit the weights with the closed-form solution of ridge regression.\n",
        "  Args:\n",
        "    x: Input of size (N, D)\n",
        "    y: Output of size (N,)\n",
        "    lam: Regularization parameter lambda\n",
        "    degree: Polynomial degree\n",
        "  Returns:\n",
        "    Optimal weights\n",
        "  \"\"\"\n",
        "  #### YOUR SOLUTION HERE\n",
        "\n",
        "def predict(x, w, degree):\n",
        "  \"\"\"\n",
        "  Calculate the generalized linear regression estimate w^T phi(x) given x,\n",
        "  the feature function, and weights w.\n",
        "  Args:\n",
        "    x: input of size (N, D)\n",
        "    w: Weights of size (M)\n",
        "    degree: Polynomial degree\n",
        "  Returns:\n",
        "    Generalized linear regression estimate\n",
        "  \"\"\"\n",
        "  #### YOUR SOLUTION HERE\n",
        "\n",
        "def calc_mse(x, y):\n",
        "  \"\"\"\n",
        "  Calculates the mean squared error (MSE) between x and y\n",
        "  Args:\n",
        "    x: Data x of size (N,)\n",
        "    y: Data y of size (N,)\n",
        "  Returns:\n",
        "    MSE\n",
        "  \"\"\"\n",
        "  #### YOUR SOLUTION HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "03cbe561",
      "metadata": {
        "id": "03cbe561"
      },
      "source": [
        "Here you can try out your code by simply running the following cell. This cell will carry out your ridge regression implementation from above for $\\lambda=0$ in which case we are provided with the linear least squares solution.\n",
        "\n",
        "We evaluate the regression task on six different polynomial sizes $k = \\{0,1,3,5,7,9\\}$ based on your implementation of the MSE."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8d2deb8",
      "metadata": {
        "id": "b8d2deb8"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "# Settings\n",
        "n_train = 15\n",
        "n_test = 100\n",
        "minval = -2.\n",
        "maxval = 2\n",
        "\n",
        "\n",
        "train_data = generate_data(n_train, minval, maxval, train=True, seed=1001)\n",
        "test_data = generate_data(n_test, minval, maxval, train=False, seed=1002)\n",
        "\n",
        "def plot_linear_regression(x, y, labels, eval_quantity):\n",
        "  \"\"\"Plotting functionality for the prediction of linear regression\n",
        "  for K different polynomial degrees.\n",
        "  Args:\n",
        "    x: Data of size (K, N). The first dimension denotes the different\n",
        "      polynomial degrees that has been experimented with\n",
        "    y: Data of size (K, N)\n",
        "  \"\"\"\n",
        "  K = x.shape[0]\n",
        "  colors = matplotlib.colormaps['Reds'].resampled(K+1)(range(1, K+1))\n",
        "  fig = plt.figure()\n",
        "  plt.scatter(test_data[0], test_data[1], color=\"tab:orange\", linewidths=0.5, label=\"Test\", alpha=0.3)\n",
        "  plt.scatter(train_data[0], train_data[1], color=\"tab:blue\", linewidths=0.5, label=\"Train\", alpha=0.3)\n",
        "  for i in range(K):\n",
        "    plt.plot(x[i], y[i], label=f\"{eval_quantity}={labels[i]}\", color=colors[i], lw=2.5)\n",
        "  plt.xlabel(\"x\")\n",
        "  plt.ylabel(\"y\")\n",
        "  plt.legend()\n",
        "\n",
        "def plot_mse(mse, labels):\n",
        "  \"\"\"Plotting functionality of the MSE for K different polynomial degrees.\"\"\"\n",
        "  fig = plt.figure()\n",
        "  plt.plot(labels, mse)\n",
        "  plt.scatter(labels, mse)\n",
        "  plt.xticks(labels)\n",
        "  plt.ylabel(\"MSE\")\n",
        "  plt.xlabel(\"Polynomial Degree\")\n",
        "\n",
        "# Evaluate regression for different polynomial degrees\n",
        "degrees = [0, 1, 3, 5, 7, 9]\n",
        "xs, ys, mse = [], [], []\n",
        "for degree in degrees:\n",
        "  w = fit_w(\n",
        "      train_data[0], train_data[1],\n",
        "      lam=0., # Edge case resulting in linear least squares regression\n",
        "      degree=degree\n",
        "  )\n",
        "  # Predict the test data\n",
        "  y_test = predict(test_data[0], w, degree)\n",
        "  mse.append(calc_mse(y_test, test_data[1]))\n",
        "\n",
        "  # Run regression over the whole interval\n",
        "  xs.append(np.linspace(minval, maxval, 100))\n",
        "  ys.append(predict(xs[-1], w, degree))\n",
        "xs = np.stack(xs)\n",
        "ys = np.stack(ys)\n",
        "\n",
        "plot_linear_regression(xs, ys, labels=degrees, eval_quantity=\"k\")\n",
        "plot_mse(mse, degrees)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3610befd",
      "metadata": {
        "id": "3610befd"
      },
      "source": [
        "## Question 1.2: Interpretation of the results\n",
        "Please describe your results below in a few lines thereby answering which model you would choose and which phenomenon we see for small and large polynomial degrees."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90044385",
      "metadata": {
        "id": "90044385"
      },
      "source": [
        "#### YOUR SOLUTION HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fee73e0",
      "metadata": {
        "id": "4fee73e0"
      },
      "source": [
        "## Question 1.3: Bias Variance Tradeoff in Linear Regression\n",
        "\n",
        "The MSE can be used to quantify the performance of our estimator, $\\hat{f}$. The MSE can be decomposed into a form involving the bias and variance, giving rise to the bias-variance tradeoff. Suppose we would like to fit a model to regress on a query datapoint ($y(\\bm{x}_q), \\bm{x}_q$):\n",
        "\n",
        "$$\n",
        "y(\\bm{x}_q) = f(\\bm{x}_q) + \\epsilon\n",
        "$$\n",
        "\n",
        "where $\\mathbb{E}[\\epsilon] = 0$ and $Var[\\epsilon] = \\sigma_\\epsilon^2$. The loss is quantified as\n",
        "\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\mathcal{L}_{\\hat{f}}(\\bm{x}_q) &= \\sigma^2_{\\epsilon} + \\mathbb{E}_{\\mathcal{D}, \\epsilon} \\left[ (y(\\bm{x}_q) - \\hat{f}_{\\mathcal{D}}(\\bm{x}_q))^2 \\right] \\\\\n",
        "\\end{align*}\n",
        "$$\n",
        "\n",
        "Show that this can be reduced to sum of the bias and variance, i.e.:\n",
        "\n",
        "$$\n",
        "\\mathcal{L}_{\\hat{f}}(\\bm{x}_q) = \\sigma^2_{\\epsilon} + \\text{bias}^2 + \\text{var}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d6b69ce",
      "metadata": {
        "id": "3d6b69ce"
      },
      "source": [
        "#### YOUR SOLUTION HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdeb750a",
      "metadata": {
        "id": "cdeb750a"
      },
      "source": [
        "## Question 1.4: Empirical evaluation of the bias and the variance\n",
        "The bias-variance tradeoff is typically a purely theoretical concept as it requires the evaluation of $f(x)$ which is commonly not available. In this task we assume that $f(x)$ is known and thus, an approximation of the bias and variance is possible. Instead of the pointwise bias and variance based on a query $\\bm{x}_q$, we introduce the lecture bias and variance as an expected value over an evaluation dataset $\\mathcal{D}^\\prime$.\n",
        "$$\n",
        "\\begin{aligned}\n",
        "    \\text{Bias}(\\hat{f}) &= \\mathbb{E}_{\\bm{x}_q \\sim \\mathcal{D^\\prime}}[\\text{bias}^2(\\hat{f}(\\bm{x}_q))] &&= \\mathbb{E}_{\\bm{x}_q \\sim \\mathcal{D^\\prime}}[f(\\bm{x}_q) - \\bar{\\hat{f}}(\\bm{x}_q)],\\\\\n",
        "    \\text{Var}(\\hat{f}) &= \\mathbb{E}_{\\bm{x}_q \\sim \\mathcal{D^\\prime}}[\\text{var}(\\hat{f}(\\bm{x}_q))] &&= \\mathbb{E}_{\\bm{x}_q \\sim \\mathcal{D^\\prime}}[\\mathbb{E}_{\\tilde{\\mathcal{D}} \\subset\\mathcal{D}}[(\\bar{\\hat{f}}(\\bm{x}_q) - \\hat{f}_{\\tilde{\\mathcal{D}}}(\\bm{x}_q))^2]].\n",
        "\\end{aligned}\n",
        "$$\n",
        "Here, $\\bar{\\hat{f}}$ is the average prediction of the maximum likelihood estimator over various datasets sampled from the ground truth distribution\n",
        "$$\\bar{\\hat{f}}(\\bm{x}_q) = \\mathbb{E}_{\\tilde{\\mathcal{D}} \\subset\\mathcal{D}}[f_{\\tilde{\\mathcal{D}}}(\\bm{x}_q)].$$\n",
        "\n",
        "To empirically study the effect of the Bias and the Var, we approximate the expectations with Monte Carlo samples as follows\n",
        "$$\\bar{\\hat{f}}(x) \\approx \\frac{1}{M}\\sum_{j=1}^{M}\\left(\\hat{f}_{\\mathcal{D_j}}(x)\\right).$$\n",
        "\n",
        "$$\\text{Bias}[\\hat{f}_\\mathcal{D}] \\approx \\frac{1}{N}\\sum_{i=1}^{N}\\left(f(x_i) - \\bar{\\hat{f}}(x_i)\\right)^2,$$\n",
        "\n",
        "$$\\text{Var}\\left[\\hat{f}_\\mathcal{D}\\right] \\approx \\frac{1}{NM}\\sum_{i=1}^{N}\\sum_{j=1}^{M}\\left(\\hat{f}_{\\mathcal{D}_j}(x_i) - \\bar{\\hat{f}}(x_i)\\right)^2$$\n",
        "\n",
        "Note that the indice $j=1,\\dots,M$ refer to subsets $\\tilde{\\mathcal{D}}_i$ from the dataset $\\mathcal{D}$, whereas the samples $x_i$ from the outer expectation relate to the indices denoted by $i=1,\\dots,N$ from an evaluation dataset $\\mathcal{D}^\\prime$.\n",
        "\n",
        "In the following exercise, please implement the Monte Carlo approximations for the average prediction, the Bias, and the Variance as shown above.\n",
        "\n",
        "Hint: You can and absolutely should use the `predict` function from the regression example as well as the function of the true model `f` from above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca0c5782",
      "metadata": {
        "id": "ca0c5782"
      },
      "outputs": [],
      "source": [
        "def avg_prediction(x, ws, degree):\n",
        "  \"\"\"\n",
        "  Approximation of the average prediction using the M function estimations\n",
        "  Args:\n",
        "    x: Input data of size (N,). N denotes the number of samples in the evaluation dataset.\n",
        "    ws: The weights obtained from ridge regression of size (M, degree).\n",
        "      The first dimension denotes the M different subset datasets on which the estimator is fitted.\n",
        "    degree: The polynomial degree\n",
        "  Returns:\n",
        "    The average prediction as a scalar\n",
        "  \"\"\"\n",
        "  #### YOUR SOLUTION HERE\n",
        "\n",
        "\n",
        "def calc_bias(x_q, ws, degree):\n",
        "  \"\"\"Estimate the bias.\n",
        "  Args:\n",
        "    x_q: Input data of size (N,). N denotes the number of samples in the evaluation dataset.\n",
        "    ws: The weights obtained from ridge regression of size (M, degree)\n",
        "      The first dimension denotes the M different subset datasets on which the estimator is fitted.\n",
        "    degree: The polynomial degree\n",
        "  Returns:\n",
        "    Model Bias as a scalar\n",
        "  \"\"\"\n",
        "  #### YOUR SOLUTION HERE\n",
        "\n",
        "def calc_variance(x_q, ws, degree):\n",
        "  \"\"\"Estimate the model variance\n",
        "  Args:\n",
        "    x_q: Input data of size (N,). N denotes the number of samples in the evaluation dataset.\n",
        "    ws: The weights obtained from ridge regression of size (M, degree)\n",
        "      The first dimension denotes the M different subset datasets on which the estimator is fitted.\n",
        "    degree: The polynomial degree\n",
        "  Returns:\n",
        "    Model variance as a scalar\n",
        "  \"\"\"\n",
        "  #### YOUR SOLUTION HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15ba6ca1",
      "metadata": {
        "id": "15ba6ca1"
      },
      "source": [
        "You can test your implementation by running the below coding snippet. It estimate the bias and variance for $M = 25$ datasets with each dataset containing $N=20$ datapoints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76aeea6d",
      "metadata": {
        "id": "76aeea6d"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "# Settings\n",
        "n = 20\n",
        "m = 25\n",
        "minval = -2.\n",
        "maxval = 2.\n",
        "degree = 9\n",
        "train_datasets = []\n",
        "seed = 3001\n",
        "for i in range(m):\n",
        "  train_datasets.append(generate_data(n_train, minval, maxval, train=True, seed=seed))\n",
        "  seed += 1\n",
        "eval_points = np.linspace(minval, maxval, n)\n",
        "\n",
        "# Estimate the bias and variance\n",
        "biases = []\n",
        "vars = []\n",
        "xs, ys = [], []\n",
        "lambdas = [1e-6, 1e-4, 1e-2, 1e0, 1e2, 1e4, 1e6]\n",
        "# Evaluate the different models, characterized by the regularization parameter lambda\n",
        "for l in lambdas:\n",
        "  w_maps = []\n",
        "  for data in train_datasets:\n",
        "    # Fit the estimator on each individual dataset\n",
        "    w = fit_w(x=data[0], y=data[1], lam=l, degree=degree)\n",
        "    w_maps.append(w)\n",
        "  # Estimate the bias of the model\n",
        "  bias = calc_bias(eval_points, w_maps, degree)\n",
        "  biases.append(bias)\n",
        "  # Estimate the variance of the model\n",
        "  var = calc_variance(eval_points, w_maps, degree)\n",
        "  vars.append(var)\n",
        "  xs.append(np.linspace(minval, maxval, 100))\n",
        "  ys.append(predict(xs[-1], w_maps[0], degree))\n",
        "\n",
        "biases = np.array(biases)\n",
        "vars = np.array(vars)\n",
        "xs = np.stack(xs)\n",
        "ys = np.stack(ys)\n",
        "\n",
        "# Plot the bias and variance for different lambas\n",
        "plt.figure()\n",
        "plt.plot(lambdas, biases, label=\"Bias\")\n",
        "plt.plot(lambdas, vars, label=\"Variance\")\n",
        "plt.plot(lambdas, biases + vars, label=\"Total Err.\")\n",
        "plt.xscale(\"log\")\n",
        "plt.xlabel(r\"$\\lambda$\")\n",
        "plt.ylabel(\"Error\")\n",
        "plt.legend()\n",
        "\n",
        "# Calculate predictions\n",
        "plot_linear_regression(xs, ys, labels=lambdas, eval_quantity=r\"$\\lambda$\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8906acc",
      "metadata": {
        "id": "a8906acc"
      },
      "source": [
        "## Question 1.5: Interpretation of the bias and variance trade-off\n",
        "Please explain the results in a few sentences. In particular, provide an explanation if the bias and variance behave as expected. For which regularization parameter $\\lambda$ would you decide?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e499518b65413e47",
      "metadata": {
        "id": "e499518b65413e47"
      },
      "source": [
        "#### YOUR SOLUTION HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16bcdebb",
      "metadata": {
        "id": "16bcdebb"
      },
      "source": [
        "# Part 2: Clustering using Parametric Density Estimation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8499b341",
      "metadata": {
        "id": "8499b341"
      },
      "source": [
        "## Question 2.1: Visualization of the Dataset\n",
        "Before being able to cluster the dataset, you need to obtain an initial impression of the data to cluster. Please generate a scatter plot of the data. What is the optimal number of clusters for the dataset?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0bb9c73",
      "metadata": {
        "id": "e0bb9c73"
      },
      "outputs": [],
      "source": [
        "#### YOUR SOLUTION HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44d7cccd",
      "metadata": {
        "id": "44d7cccd"
      },
      "source": [
        "Please set the optimal number of clusters based on your empirical observation of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "400cb6df",
      "metadata": {
        "id": "400cb6df"
      },
      "outputs": [],
      "source": [
        "OPTIMAL_NUM_CLUSTERS = (\n",
        "    #### YOUR SOLUTION HERE\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6987f989",
      "metadata": {
        "id": "6987f989"
      },
      "source": [
        "## Question 2.2: Implementation of the EM Algorithm\n",
        "In the lecture about parametric density estimation, you got introduced to the EM Algorithm to be able to fit a Gaussian Mixture Model (GMM) over your dataset. Please implement the EM algorithm from scratch by filling in the following functions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "224d8c7d",
      "metadata": {
        "id": "224d8c7d"
      },
      "outputs": [],
      "source": [
        "def e_step(data, mixture_weights, means, covariances):\n",
        "    \"\"\"\n",
        "    :param data: all data points\n",
        "    :param mixture_weights: mixture weights of Gaussian Mixture Model\n",
        "    :param means: means of Gaussian Mixture Model\n",
        "    :param covariances: covariances of Gaussian Mixture Model\n",
        "\n",
        "    :return: responsibilities of shape (num_data_samples, num_clusters)\n",
        "    \"\"\"\n",
        "    #### YOUR SOLUTION HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e51d3db4",
      "metadata": {
        "id": "e51d3db4"
      },
      "outputs": [],
      "source": [
        "def m_step(data, responsibilities):\n",
        "    \"\"\"\n",
        "    :param data: all data points\n",
        "    :param responsibilities: responsibilities from e_step\n",
        "\n",
        "    :return: updated mixture_weights, updated means, updated covariances\n",
        "    \"\"\"\n",
        "    #### YOUR SOLUTION HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2fa78cec",
      "metadata": {
        "id": "2fa78cec"
      },
      "outputs": [],
      "source": [
        "def log_likelihood(data, mixture_weights, means, covariances):\n",
        "    \"\"\"\n",
        "    :param data: all data points\n",
        "    :param mixture_weights: mixture weights of Gaussian Mixture Model\n",
        "    :param means: means of Gaussian Mixture Model\n",
        "    :param covariances: covariances of Gaussian Mixture Model\n",
        "\n",
        "    :return: calculated log likelihood\n",
        "    \"\"\"\n",
        "\n",
        "    #### YOUR SOLUTION HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de2b2d18",
      "metadata": {
        "id": "de2b2d18"
      },
      "source": [
        "Now, implement the expectation_maximization function which leverages the previously implemented functions. Given initial values of the mixture weights, means and covariances, you should apply the E- and M-Step until the log-likelihood has converged, i.e. the absolute difference in subsequent values is below a certain threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89a6c255",
      "metadata": {
        "id": "89a6c255"
      },
      "outputs": [],
      "source": [
        "def expectation_maximization(data, init_mixture_weights, init_means, init_covariances, threshold):\n",
        "    \"\"\"\n",
        "    :param data: all data points\n",
        "    :param init_mixture_weights: initial mixture weights of Gaussian Mixture Model\n",
        "    :param init_means: initial means of Gaussian Mixture Model\n",
        "    :param init_covariances: initial covariances of Gaussian Mixture Model\n",
        "    :param threshold: threshold for checking convergence\n",
        "\n",
        "    :return: final mixture_weights, final means, final covariances, final responsibilities, final log_likelihood, num_iterations needed till convergence\n",
        "    \"\"\"\n",
        "    #### YOUR SOLUTION HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d03cc9e",
      "metadata": {
        "id": "0d03cc9e"
      },
      "source": [
        "## Question 2.3: Clustering the Dataset and Visualizing the Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9174722",
      "metadata": {
        "id": "e9174722"
      },
      "outputs": [],
      "source": [
        "def plot_results(data, responsibilities, means, covariances):\n",
        "    \"\"\"\n",
        "    :param data: all data points\n",
        "    :param responsibilities: final responsibilities\n",
        "    :param means: final means\n",
        "    :param covariances: final covariances\n",
        "\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "\n",
        "    num_components = len(means)\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    # label the data points w.r.t to the final responsibilities\n",
        "    labels = np.argmax(responsibilities, axis=1)\n",
        "    ax.scatter(data[:, 0], data[:, 1], c=labels)\n",
        "\n",
        "    # plot the resulting ellipsoids of mixture of Gaussians\n",
        "    for k in range(num_components):\n",
        "        eigenvalues, eigenvectors = np.linalg.eig(covariances[:,:,k])\n",
        "\n",
        "        # Sort eigenvalues and eigenvectors in descending order\n",
        "        sorted_indices = np.argsort(eigenvalues)[::-1]\n",
        "        eigenvalues = eigenvalues[sorted_indices]\n",
        "        eigenvectors = eigenvectors[:, sorted_indices]\n",
        "\n",
        "        # Calculate the angle of rotation\n",
        "        angle = np.degrees(np.arctan2(eigenvectors[1, 0], eigenvectors[0, 0]))\n",
        "\n",
        "        # Create an ellipse based on the eigenvalues and eigenvectors\n",
        "        ellipse = Ellipse(means[k], 2 * np.sqrt(eigenvalues[0]), 2 * np.sqrt(eigenvalues[1]),\n",
        "                          angle=angle, fill=False, edgecolor='r')\n",
        "\n",
        "        # Plot the ellipse\n",
        "        ax.add_patch(ellipse)\n",
        "\n",
        "    # Add labels and title\n",
        "    plt.xlabel('X')\n",
        "    plt.ylabel('Y')\n",
        "    plt.title('Clustering Result')\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96d5d0f9",
      "metadata": {
        "id": "96d5d0f9"
      },
      "source": [
        "Please implement the main function which should load the dataset, execute the EM algorithm and plot the results using the previously implemented functions. Note, the EM algorithm expects initial values for the mixture weights, means and covariances of the Gaussian Mixture Model. Please initialize the mixture weights uniformly over the number of mixture components and the initial covariances with a unit scale. You can experiment around with the initial means as you like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88092c3f",
      "metadata": {
        "id": "88092c3f"
      },
      "outputs": [],
      "source": [
        "DATASET_PATH = \"./clustering_dataset.npy\"\n",
        "\n",
        "# Hyperparameters\n",
        "NUM_MIXTURE_COMPONENTS = OPTIMAL_NUM_CLUSTERS             # as previously defined by yourself\n",
        "CONVERGENCE_THRESHOLD = 1e-6\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    :return: None\n",
        "    \"\"\"\n",
        "    #### YOUR SOLUTION HERE\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ee4a89c",
      "metadata": {
        "id": "7ee4a89c"
      },
      "source": [
        "## Question 2.4: Report Results\n",
        "Please report the following quantitative information regarding your final solution:\n",
        "- Final Mixture Weights\n",
        "- Initial Means\n",
        "- Final Means\n",
        "- Final Covariances\n",
        "- Final Log Likelihood\n",
        "- Number of Iterations until Convergence"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3903a7bc",
      "metadata": {
        "id": "3903a7bc"
      },
      "source": [
        "#### YOUR SOLUTION HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "540edb92",
      "metadata": {
        "id": "540edb92"
      },
      "source": [
        "# Part 3: Biased estimators\n",
        "For a set of i.i.d. observations $X = \\{x_1, x_2, \\dots, x_n\\}$ from a normal distribution $\\mathcal{N}(\\mu, \\sigma^2)$ with mean $\\mu$ and variance $\\sigma^2$, we know that the maximum likelihood estimate of the variance is\n",
        "$$\n",
        "\\hat{\\sigma}^2 = \\frac{1}{N} \\sum_{i=1}^{N} (x_i - \\overline{x})^2,\n",
        "$$\n",
        "where $\\overline{x} = \\frac{1}{N} \\sum_{i=1}^{N} x_i$ is the sample mean."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "621485c8",
      "metadata": {
        "id": "621485c8"
      },
      "source": [
        "## Question 3.1: Bias of the sample variance\n",
        "Please follow the steps below to show that $\\hat{\\sigma}^2$ is a biased estimate of the variance.\n",
        "\n",
        "1. Derive the expected value of $\\hat{\\sigma}^2$, i.e., $\\mathbb{E}_{X}[\\hat{\\sigma}^2]$.\n",
        "2. Show that $\\mathbb{E}_{X}[\\hat{\\sigma}^2] \\neq \\sigma^2$, indicating that the sample variance tends to underestimate the population variance.\n",
        "3. Modify the estimator to obtain an unbiased estimate of the population variance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8312b62c",
      "metadata": {
        "id": "8312b62c"
      },
      "source": [
        "#### YOUR SOLUTION HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7353b252",
      "metadata": {
        "id": "7353b252"
      },
      "source": [
        "# Part 4: Classification\n",
        "\n",
        "The lecture introduces logistic regression as a solution to a binary classification problem in which the class posterior is modeled by a linear discriminant model and a sigmoid activation function $p(C_1 | \\bm{x}) = \\sigma(\\bm{w}^\\intercal \\bm{x} + b)$. Here $w$ denotes the weight of the linear function and $b$ the shift in y-direction. Instead of following classical maximum likelihood ideas to estimate the parameters $w$ and $b$, we want to implement a very simple, yet powerful, binary classification algorithm - The perceptron algorithm. Instead of the sigmoid function, we instead use the sign function\n",
        "$$\n",
        "\\mathrm{sign}(\\bm{x}) =\\begin{cases}\n",
        "    1, \\quad \\mathrm{if}~\\bm{x}>0, \\\\\n",
        "    0, \\quad \\mathrm{if}~\\bm{x}=0, \\\\\n",
        "    -1, \\quad \\mathrm{if}~\\bm{x}<0.\n",
        "\\end{cases}\n",
        "$$\n",
        "to directly estimate the class using the discriminant function $y(x) = \\mathrm{sign}(\\bm{w}^\\intercal \\bm{x} + b)$."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "079980ff",
      "metadata": {
        "id": "079980ff"
      },
      "source": [
        "To introduce the problem, we First generate some synthetic data, where each datapoint belongs to on of two classes (-1 or 1). We further highlight the optimal discriminant function in green."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e972a198",
      "metadata": {
        "id": "e972a198"
      },
      "outputs": [],
      "source": [
        "def plot_classification(data, w, b, classes):\n",
        "    \"\"\"Helper function for plotting the data and decision boundary\"\"\"\n",
        "\n",
        "    x_min, x_max = min(data[:, 0]), max(data[:, 0])\n",
        "    y_min, y_max = min(data[:, 1]), max(data[:, 1])\n",
        "\n",
        "    x = np.linspace(x_min, x_max)\n",
        "    if w[1] == 0:\n",
        "        plt.plot(x, np.ones_like(x) * -b, color=\"green\", label=\"decision boundary\")\n",
        "    else:\n",
        "        plt.plot(x, (-b -w[0] * x) / w[1], color=\"green\", label=\"decision boundary\")\n",
        "    plt.scatter(data[classes, 0], data[classes, 1], color=\"blue\", label=\"Class 1\")\n",
        "    plt.scatter(data[~classes, 0], data[~classes, 1], color=\"red\", label=\"Class 2\")\n",
        "    plt.xlim((x_min, x_max))\n",
        "    plt.ylim((y_min, y_max))\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98ce3626",
      "metadata": {
        "id": "98ce3626"
      },
      "outputs": [],
      "source": [
        "data = (2 * np.random.random(size=(50, 2)) - 1) * 10\n",
        "w_theoric = np.array([1, 2])\n",
        "b_theoric = 2\n",
        "\n",
        "classes = (data @ w_theoric.T + b_theoric) > 0\n",
        "labels = 2 * classes.astype(int) - 1\n",
        "\n",
        "plot_classification(data, w_theoric, b_theoric, classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "319bb9bb",
      "metadata": {
        "id": "319bb9bb"
      },
      "source": [
        "## Question 4.1: The perceptron algorithm\n",
        "\n",
        "The perceptron algorithm is a simple algorithm to estimate the weights $w$ and $b$ for the perceptron discriminant function $y(\\bm{x}) = \\mathrm{sign}(\\bm{w}^\\intercal \\bm{x} + b)$.\n",
        "\n",
        "Above we plot the data as well as the ground truth linear decision boundary which was used to generate the data.Naturally the question arises \"If we don't know the ground truth paramethers, how can we find them?\"\n",
        "For this, you should implement the perceptron algorithm within the next function.\n",
        "\n",
        "### Perceptron algorithm\n",
        "- Inititalize the weight $w$ and bias $b$\n",
        "- For all pairs of data points $(\\bm{x}_i, y_i)$, repeat until convergence\n",
        "    - If $x_i$ is correctly classified, i.e., $y(\\bm{x}_i) = y_i$, do nothing,\n",
        "    - else if $y_i = 1$ update the parameters with $\\bm{w} \\leftarrow \\bm{w} + \\gamma \\bm{x}_i, b \\leftarrow b + 1$,\n",
        "    - else if $y_i = -1$ update the parameters with $\\bm{w} \\leftarrow \\bm{w} - \\gamma \\bm{x}_i, b \\leftarrow b - 1$\n",
        "\n",
        "Here, $\\gamma$ ia a learning rate which balances how much the weight is moved towards the sample $x_i$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c57599c6",
      "metadata": {
        "id": "c57599c6"
      },
      "outputs": [],
      "source": [
        "def perceptron(data, labels, iterations=100, learning_rate=0.1):\n",
        "\n",
        "    w = np.zeros(2)\n",
        "    b = 0\n",
        "\n",
        "    print('Initial')\n",
        "    plot_classification(data, w, b, classes)\n",
        "\n",
        "    for i in range(iterations):\n",
        "\n",
        "        #### YOUR SOLUTION HERE\n",
        "\n",
        "    print('Final solution')\n",
        "    plot_classification(data, w, b, classes)\n",
        "    return w, b\n",
        "\n",
        "w,b = perceptron(data, labels, 10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a839b52d",
      "metadata": {
        "id": "a839b52d"
      },
      "source": [
        "### Question 4.2: Intuition on the update steps\n",
        "Explain intuitively how the update steps in the perceptron algorithm are similar to gradient descent, and why they help the model improve its classification decision over time."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "005c9a97",
      "metadata": {
        "id": "005c9a97"
      },
      "source": [
        "#### YOUR SOLUTION HERE"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}